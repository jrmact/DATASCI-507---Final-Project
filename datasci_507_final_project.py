# -*- coding: utf-8 -*-
"""DATASCI 507 - Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pMuVcU6pf1UbgLXki1k-Ug7B3Y7wZrc8

# Data Processing
"""

import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import torch
from torch import nn
from torch import optim
import torchvision
from torchvision import transforms
from torch.utils.data import random_split, DataLoader
from sklearn.preprocessing import StandardScaler

# RNG
torch.manual_seed(42)

"""Because of instrumental errors in 2011 from an event that affected ACE, certain data is tainted. This following cell fixes this problem by appropriately filtering the data."""

import scipy.io
from datetime import datetime, timedelta
import pandas as pd

# Load the IDL .save file
data = scipy.io.readsav('ACE_SWICS_1998_2022_COratio_combined.save')

# Explore the data structure to understand how data is stored
#print(data.keys())

ace_swics = pd.DataFrame({
    'fyear_ace': data['fyear_ace'],
    'o7too6_t': data['o7too6_t'],
    'c6toc5_t': data['c6toc5_t'],
    'h_v': data['h_v']
})

ace_swics['fyear_ace'] = ace_swics['fyear_ace'].astype(np.float64, copy=False)
ace_swics['o7too6_t'] = ace_swics['o7too6_t'].astype(np.float32, copy=False)
ace_swics['c6toc5_t'] = ace_swics['c6toc5_t'].astype(np.float32, copy=False)
ace_swics['h_v'] = ace_swics['h_v'].astype(np.float32, copy=False)

ace_swics = ace_swics[(ace_swics['o7too6_t'] < 0.145) & (ace_swics['o7too6_t'] > 0.052272) & (ace_swics['c6toc5_t'] <= 1.425) & (ace_swics['c6toc5_t'] >= 0.1104)]

def fractional_year_to_datetime(fractional_year):
    """Convert a fractional year to a datetime object."""
    # Separate the integer year from the fractional part
    year = int(fractional_year)
    fractional_part = fractional_year - year

    # Calculate the number of days corresponding to the fractional part
    # Consider if it's a leap year for precise calculations
    base = datetime(year, 1, 1)
    if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):
        # Leap year: 366 days
        days_in_year = 366
    else:
        # Non-leap year: 365 days
        days_in_year = 365

    # Compute the exact date by adding the number of whole days represented
    # by the fractional part.
    delta = timedelta(days=fractional_part * days_in_year)
    return base + delta

ace_swics['fyear_ace'] = ace_swics['fyear_ace'].apply(fractional_year_to_datetime)
ace_swics.rename(columns={'fyear_ace':'Datetime'}, inplace=True)

ace_swics.set_index('Datetime', inplace=True)

ace_swics.dropna(inplace=True)
ace_swics = ace_swics.resample('D').mean()

#ace_swics

mag_dataframe=pd.read_csv("ACE_MAG_FULL.txt", header=37, sep="\s+")
mag_dataframe['md'] = pd.to_datetime(mag_dataframe['day'], format='%j').dt.strftime('%m-%d')
mag_dataframe['time'] = pd.to_datetime(mag_dataframe['hr'], format='%H').dt.strftime('%H:%M:%S')
mag_dataframe['Datetime'] = pd.to_datetime(mag_dataframe['year'].astype(str) + '-' + mag_dataframe.md + " " + mag_dataframe.time)
mag_dataframe = mag_dataframe.replace(-999.900, np.nan)
mag_dataframe.dropna(subset=['Br'], inplace=True)
mag_dataframe['|Br|r^2'] = abs(mag_dataframe['Br'])
mag_dataframe.index = mag_dataframe.Datetime
mag_dataframe = mag_dataframe.drop(columns=['year','day','hr','min','sec','fp_year','fp_doy','Datetime','md','time'])
mag_dataframe = mag_dataframe.resample('D').mean()

#mag_dataframe

ace = mag_dataframe[mag_dataframe.index.isin(ace_swics.index)]
#ace

hcs = pd.read_csv("ACE_HCS.csv")

hcs["Datetime"] = hcs["fyear"].apply(fractional_year_to_datetime)
hcs["Polar Angle"] = (90 - np.abs(hcs["distance to HCS in degree"])) * np.pi / 180

hcs["Non-Streamer Area"] = 4 * np.pi * (1 - np.cos(hcs["Polar Angle"]))

hcs.index = hcs.Datetime
hcs.drop(columns=["fyear", "Datetime"], inplace=True)

hcs = hcs.resample('D').mean()

#hcs

ace = ace.join(hcs, how='inner')
#ace

ace["TOMF"] = ace["|Br|r^2"] * ace["Non-Streamer Area"]
#ace

ace = ace.join(ace_swics, how='inner')
#ace

ace.drop(columns=["ACEepoch", "SCclock", "Delta", "Lambda", "fraction_good", "N_vectors", "Quality", "pos_gse_x", "pos_gse_y", "pos_gse_z", "pos_gsm_x", "pos_gsm_y", "pos_gsm_z"], inplace=True)
ace.columns

#data leakage columns used to calculate TOMF must be removed
ace.drop(columns=["Br", "|Br|r^2", 'distance to HCS in degree', 'Polar Angle', 'Non-Streamer Area'], inplace=True)
ace.columns

!python -m pip install hapiclient

from dateutil.relativedelta import relativedelta
from hapiclient import hapi, hapitime2datetime

server     = 'https://cdaweb.gsfc.nasa.gov/hapi'
dataset    = 'OMNI2_H0_MRG1HR'
# Notes:
# 1. Use parameters='' to request all parameters from OMNI2_H0_MRG1HR.
# 2. Multiple parameters can be requested using a comma-separated
#    list, e.g., parameters='Rot1800,IMF1800'
parameters = 'R1800'
start      = '1970-01-01T00:00:00Z' # min 1963-01-01T00:00:00Z
stop       = '2023-05-19T00:00:00Z' # max 2024-06-05T14:00:00Z

ssn_data2, ssn_meta = hapi(server, dataset, parameters, start, stop)

ssnData = pd.DataFrame(ssn_data2)

ssnData = ssnData.replace(-1, np.nan)

ssnData["Datetime"] = hapitime2datetime(ssn_data2["Time"])
ssnData['Datetime'] = ssnData['Datetime'].apply(lambda t: t.replace(tzinfo=None))
ssnData.rename(columns={"R1800":"ssn"}, inplace = True)
ssn_data1 = ssnData
print(ssn_data1.iloc[78])

ssnData.index = ssnData["Datetime"]

ssnData = ssnData.drop(columns=["Time", "Datetime"])

ssnData = ssnData.resample("D").mean()

#ssnData

ace = ace.join(ssnData, how='inner')
#ace

ace=ace.dropna()
#ace

"""# Split Data for Testing"""

ace = ace.reset_index()
ace['Year'] = ace['Datetime'].dt.year
ace['Month'] = ace['Datetime'].dt.month
ace['Day'] = ace['Datetime'].dt.day

feature_cols = ['Bt', 'Bn', 'Bmag', 'Bgse_x', 'Bgse_y', 'Bgse_z',
                'dBrms', 'sigma_B', 'o7too6_t', 'c6toc5_t', 'h_v',
                'Year', 'Month', 'Day', 'ssn']

X = ace[feature_cols]
y = ace['TOMF']

scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=feature_cols)
X.index = ace['Datetime']

scaler_y = StandardScaler()
y = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

from sklearn.model_selection import train_test_split
from torch.utils.data import TensorDataset

# Split data into training and test sets (80% train, 20% test+val)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# Further split the temporary set into validation and test sets (10% each)
#X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)

# Convert training, validation, and test data to PyTorch tensors
# Use the values attribute of the DataFrame to get the underlying NumPy array
X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
#X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
#y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Create TensorDatasets
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
#test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Shuffle for training
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)     # No need to shuffle
#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

"""# Machine Learning Models

## Trees
"""

from sklearn.metrics import mean_squared_error

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn import tree

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, mean_squared_error

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier

"""### CART"""

cart = DecisionTreeRegressor(max_depth=4, random_state=42)
cart.fit(X_train, y_train)

y_pred_cart = cart.predict(X_val)
mse_cart = mean_squared_error(y_val, y_pred_cart)
print(f"Mean Squared Error: {mse_cart}")

plt.figure(figsize=(20,10))
tree.plot_tree(cart, feature_names=feature_cols, filled=True)
plt.show()

"""### Random Forest"""

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_val)
mse_rf = mean_squared_error(y_val, y_pred_rf)
print(f"Mean Squared Error: {mse_rf}")

plt.figure(figsize=(20,10))
tree.plot_tree(rf.estimators_[0], feature_names=feature_cols, filled=True)
plt.show()

"""### Boosting"""

boost = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
boost.fit(X_train, y_train)

y_pred_boost = boost.predict(X_val)
mse_boost = mean_squared_error(y_val, y_pred_boost)
print(f"Mean Squared Error: {mse_boost}")

plt.figure(figsize=(20,10))
tree.plot_tree(boost.estimators_[0][0], feature_names=feature_cols, filled=True)
plt.show()

"""### Tree Error Analysis"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# CART model evaluation
y_pred_cart = cart.predict(X_val)
mse_cart = mean_squared_error(y_val, y_pred_cart)
rmse_cart = np.sqrt(mse_cart)  # Calculate RMSE
mae_cart = mean_absolute_error(y_val, y_pred_cart)
r2_cart = r2_score(y_val, y_pred_cart)

# Random Forest model evaluation
y_pred_rf = rf.predict(X_val)
mse_rf = mean_squared_error(y_val, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)  # Calculate RMSE
mae_rf = mean_absolute_error(y_val, y_pred_rf)
r2_rf = r2_score(y_val, y_pred_rf)

#Boosting model evaluation
y_pred_boost = boost.predict(X_val)
mse_boost = mean_squared_error(y_val, y_pred_boost)
rmse_boost = np.sqrt(mse_boost)  # Calculate RMSE
mae_boost = mean_absolute_error(y_val, y_pred_boost)
r2_boost = r2_score(y_val, y_pred_boost)

# Create a dictionary to store the evaluation metrics
error_analysis = {
    'Model': ['CART', 'Random Forest', 'Boosting'],
    'MSE': [mse_cart, mse_rf, mse_boost],
    'RMSE': [rmse_cart, rmse_rf, rmse_boost],
    'MAE': [mae_cart, mae_rf, mae_boost],
    'R^2': [r2_cart, r2_rf, r2_boost]
}

# Create the pandas DataFrame
df_error_analysis = pd.DataFrame(error_analysis)
df_error_analysis

from sklearn.model_selection import cross_val_score, KFold

# Define the number of folds for cross-validation
n_splits = 5

# Define the cross-validation strategy (KFold)
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Perform cross-validation for each model
models = {
    'CART': cart,
    'Random Forest': rf,
    'Boosting': boost
}
results = {}

for model_name, model in models.items():

    # Calculate cross-validation scores (e.g., R-squared)
    scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')

    results[model_name] = scores
    print(f"Cross-validation results for {model_name}:")
    print(f"MSE scores: {-scores}")  # Multiply by -1 to convert from negative to positive values
    print(f"Average MSE: {-scores.mean()}")
    print(f"Standard deviation of MSE: {scores.std()}\n")

"""## Ridge Regression"""

from sklearn.linear_model import RidgeCV

alphas = np.logspace(-3, 3, 50)

ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)
ridge_cv.fit(X_train, y_train)

y_pred_ridge = ridge_cv.predict(X_val)
mse_ridge = mean_squared_error(y_val, y_pred_ridge)

print(f"Mean Squared Error: {mse_ridge}")
print(f"Best alpha: {ridge_cv.alpha_}")

"""# Building a Neural Network/Model Training"""

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10):
    """
      model (torch.nn.Module): The neural network
      train_loader (torch.utils.data.DataLoader): Training data
      val_loader (torch.utils.data.DataLoader): Validation data
      criterion (torch.nn Loss function): The loss function
      optimizer (torch.optim): The optimizer used to make gradient steps
      num_epochs (int): The number of training epochs to perform
      patience (int): Number of epochs to use for early stopping. If 0, early stopping is not used.
    """

    # intiate two lists way to keep track of training and validation losses
    validation_losses = []
    training_losses = []

    min_validation_loss = float('inf')
    patience_counter = 0

    # for each epoch...
    for epoch in range(num_epochs):
        # initialize the training loss as zero
        train_loss = 0
        # set the model to train mode
        model.train()

        for batch_X, batch_y in train_loader:
            # new iteration so we set gradients to zero
            optimizer.zero_grad()

            # get outputs from the model (forward pass)
            outputs = model(batch_X)

            # get the loss using the outputs and the truth
            loss = criterion(outputs, batch_y)

            # compute gradients of the loss with respect to model parameters (backward pass)
            loss.backward()

            # take a step with optimizer to update model parameters using computed gradients
            optimizer.step()

            # add to the epoch's runnning training loss
            train_loss += loss.item()


        # get the mean training loss
        train_loss /= len(train_loader)

        # append training loss observation to train lost list
        training_losses.append(train_loss)


        # we want to evaluate model losses on validation set so we set model
        # to eval mode here
        model.eval()

        # initialize the validation loss as zero
        val_loss = 0
        # we also don't want to do backpropagation while not training so we
        # turn off gradient
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                # val forward pass
                outputs = model(batch_X)

                # val loss function
                loss = criterion(outputs, batch_y)

                # add to total running validation loss
                val_loss += loss.item()


        # get the mean training loss
        val_loss /= len(val_loader)


        # append training loss observation to train lost list
        validation_losses.append(val_loss)

        # once in a while...
        if epoch % 10 == 0:
            # Print the epoch progress, training loss (4 decimal places), and validation loss (4 decimal places)
            print(f"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        # Early Stopping Check
        if patience > 0:
            if val_loss < min_validation_loss:
                min_validation_loss = val_loss
                patience_counter = 0  # Reset patience counter if validation loss improves
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"Early stopping triggered at epoch {epoch + 1}")
                    break  # Stop training

    return training_losses, validation_losses, model

def build_model(n_hidden=1, n_neurons=30, activation='relu', input_dim=0, output_dim=0, include_dropout=True):
    activation_map = {
        'relu': nn.ReLU,
        'tanh': nn.Tanh,
        'sigmoid': nn.Sigmoid,
        'leaky_relu': nn.LeakyReLU,
    }
    if activation not in activation_map:
        raise ValueError(f"Invalid activation function: {activation}")

    layers = [nn.Linear(input_dim, n_neurons), activation_map[activation]()]
    for _ in range(n_hidden):
        layers.extend([nn.Linear(n_neurons, n_neurons), activation_map[activation]()])

    layers.append(nn.Linear(n_neurons, output_dim))

    if include_dropout:
        layers.append(nn.Dropout(0.2))
        layers.append(nn.BatchNorm1d(output_dim))

    return nn.Sequential(*layers)

"""# Testing the Neural Network"""

!pip install optuna

import optuna
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

def objective(trial):
    n_hidden = trial.suggest_int('n_hidden', 6, 9)
    n_neurons = trial.suggest_int('n_neurons', 32, 256)
    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid', 'leaky_relu'])
    include_dropout = trial.suggest_categorical('include_dropout', [True, False])

    # Build the model
    build = build_model(n_hidden=n_hidden,
                        n_neurons=n_neurons,
                        activation=activation,
                        input_dim=15,  # Adjust as necessary
                        output_dim=1,
                        include_dropout=include_dropout)

    optimizer = optim.Adam(build.parameters(), lr=0.001, weight_decay=0.0005)
    C = nn.MSELoss()

    training_losses, validation_losses, _ = train_model(build, train_loader, val_loader, C, optimizer, num_epochs=500, patience=10)

    # Store intermediate results to access later if needed
    trial.set_user_attr('training_losses', training_losses)
    trial.set_user_attr('validation_losses', validation_losses)

    return min(validation_losses)

# Create a study and optimize it
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

# Get the best trial
best_trial = study.best_trial
best_training_losses = best_trial.user_attrs['training_losses']
best_validation_losses = best_trial.user_attrs['validation_losses']

# Plotting
plt.figure(figsize=(10, 5))
plt.plot(best_training_losses, label='Training Loss')
plt.plot(best_validation_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Losses')
plt.legend()
plt.show()

"""# GAMs"""

!pip uninstall -y pygam
!pip install numpy==2.0.2 scipy==1.14.1
!pip install git+https://github.com/dswah/pyGAM.git --quiet

from pygam import LinearGAM, s
from scipy.sparse import issparse

# Fit a GAM with a spline term for each feature
gam = LinearGAM(s(0) + s(1) + s(2) + s(3) + s(4) + s(5) +
                s(6) + s(7) + s(8) + s(9) + s(10) +
                s(11) + s(12) + s(13) + s(14))

X_train_dense = X_train.values
if issparse(X_train_dense):
    X_train_dense = X_train_dense.toarray()

y_train_array = y_train

gam.gridsearch(X_train_dense, y_train_array)

# Predictions
y_pred = gam.predict(X_val)

# Evaluation
mse = mean_squared_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)

print(f"Validation MSE: {mse:.4f}")
print(f"Validation R^2: {r2:.4f}")

plt.figure(figsize=(20, 15))
for i, term in enumerate(gam.terms):
    if term.isintercept:
        continue
    plt.subplot(4, 4, i + 1)  # Add 1 to i to start subplot indexing at 1
    XX = gam.generate_X_grid(term=i)
    plt.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))
    plt.title(f'Partial dependence: {feature_cols[i]}')
plt.tight_layout()
plt.show()